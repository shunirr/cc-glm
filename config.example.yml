# cc-glm Configuration Example
# Copy this file to ~/.config/cc-glm/config.yml and customize

# Proxy server configuration
proxy:
  port: 8787
  host: "127.0.0.1"

# Upstream API configuration
upstream:
  # Anthropic API (OAuth, forwards authorization header as-is)
  anthropic:
    url: "https://api.anthropic.com"

  # z.ai GLM API (for glm-* models)
  zai:
    url: "https://api.z.ai/api/anthropic"
    apiKey: "YOUR_API_KEY" # Or falls back to ZAI_API_KEY env var

# Lifecycle management
lifecycle:
  # Seconds to wait after Claude exits before stopping proxy
  stopGraceSeconds: 8

  # Maximum seconds to wait for proxy startup
  startWaitSeconds: 8

  # Directory for PID files and logs
  stateDir: "${TMPDIR}/claude-code-proxy"

# Logging configuration
logging:
  level: "info"  # debug, info, warn, error

# Model routing configuration
# Rules are evaluated top-to-bottom, first match wins
routing:
  rules:
    # Route Claude Sonnet requests to z.ai with GLM-4.7
    - match: "claude-sonnet-*"
      upstream: zai
      model: "GLM-4.7"

    # Route Claude Haiku requests to z.ai with GLM-4.7
    - match: "claude-haiku-*"
      upstream: zai
      model: "GLM-4.7"

    # Route GLM models to z.ai (keep original model name)
    - match: "glm-*"
      upstream: zai

  # Default upstream when no rule matches
  default: anthropic
